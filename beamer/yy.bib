@book{Zhou2014,
abstract = {"This modern introduction to seismic data processing in both exploration and global geophysics demonstrates practical applications through real data and tutorial examples. The underlying physics and mathematics of the various seismic analysis methods are presented, giving students an appreciation of their limitations and potential for creating models of the sub-surface. Designed for a one-semester course, this textbook discusses key techniques within the context of the world's ever increasing need for petroleum and mineral resources - equipping upper undergraduate and graduate students with the tools they need for a career in industry"--},
author = {Zhou, Hua-Wei},
booktitle = {Practical Seismic Data Analysis},
doi = {10.1017/CBO9781139027090},
isbn = {9780521199100},
month = {jan},
publisher = {Cambridge University Press},
title = {{Practical Seismic Data Analysis}},
url = {https://www.cambridge.org/highereducation/books/practical-seismic-data-analysis/CBDD7DF03CCA213BF6CFD8152A0F13FE{\#}contents},
year = {2014}
}
@article{Landers1979,
author = {Landers, Tom},
doi = {10.1038/277582b0},
issn = {0028-0836},
journal = {Nature},
month = {feb},
number = {5697},
pages = {582--582},
title = {{Reflection seismology}},
url = {http://www.nature.com/articles/277582b0},
volume = {277},
year = {1979}
}
@article{Uieda2017,
abstract = {Open any textbook about seismic data processing and you will inevitably find a section about the normal moveout (NMO) correction. There you'll see that we can correct the measured traveltime of a reflected wave t at a given offset x to obtain the traveltime at normal incidence t 0 by applying the following equation:},
author = {Uieda, Leonardo},
doi = {10.1190/tle36020179.1},
issn = {1070-485X},
journal = {The Leading Edge},
keywords = {Common-midpoint,NMO correction,Normal moveout,Python,Traveltime},
month = {feb},
number = {2},
pages = {179--180},
title = {{Step-by-step NMO correction}},
url = {https://library.seg.org/doi/10.1190/tle36020179.1},
volume = {36},
year = {2017}
}
@article{Mayne1962,
abstract = {Techniques are described whereby multiple coverage of the subsurface is obtained. Detector spreads and shotpoints are arranged so that the channels representing common depth points are recorded with appreciably different horizontal distances between the shotpoints and detector stations. The channels which have a common reflection point are combined, or stacked, after appropriate corrections for angularity and travel time to datum have been applied. Reflections which follow the assumed travel paths are greatly enhanced, and other events are reduced. Methods for attenuating multiple reflections with respect to primaries are discussed in considerable detail. Typical field comparisons between conventional and stacked traverses are shown to illustrate the degree of improvement which can be obtained in the signal‐to‐noise ratio. General considerations applicable to field usage, and the geographic range of field experience are summarized.},
author = {Mayne, W. Harry},
doi = {10.1190/1.1439118},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {dec},
number = {6},
pages = {927--938},
title = {{COMMON REFLECTION POINT HORIZONTAL DATA STACKING TECHNIQUES}},
url = {https://library.seg.org/doi/10.1190/1.1439118},
volume = {27},
year = {1962}
}
@article{Rashed2014,
abstract = {Common-Mid-Point (CMP) stacking is a major process to enhance signal-to-noise ratio in seismic data. Since its appearance fifty years ago, CMP stacking has gone through different phases of prosperity and negligence within the geophysical community. During those times, CMP stacking developed from a simple process of averaging into a sophisticated process that involves complicated mathematics and state-of-the-art computation. This article summarizes the basic principles, assumptions, and violations related to the CMP stacking technique, presents a historical overview on the development stages of CMP stacking, and discusses its future potentiality. {\textcopyright} 2014 Versita Warsaw and Springer-Verlag Wien.},
author = {Rashed, Mohamed},
doi = {10.2478/s11600-013-0191-4},
issn = {1895-6572},
journal = {Acta Geophysica},
keywords = {CMP,processing,seismic,stacking},
month = {jun},
number = {3},
pages = {505--528},
title = {{Fifty years of stacking}},
url = {http://link.springer.com/10.2478/s11600-013-0191-4},
volume = {62},
year = {2014}
}
@book{Sheriff1995,
author = {Sheriff, Robert E and Geldart, Lloyd P},
booktitle = {Cambridge university press},
doi = {10.1017/CBO9781139168359},
isbn = {9780521462822},
month = {aug},
publisher = {Cambridge University Press},
title = {{Exploration Seismology}},
url = {https://www.cambridge.org/core/product/identifier/9781139168359/type/book},
year = {1995}
}
@book{Mayne1989,
abstract = {This memoir was undertaken in this form because of the belief that the history of technological development in science is usually rather haphazardly (if at all) documented, particularly when compared to the evolution of parallel theory. The probable reason for this discrepancy is that a theory has to be written down; and some kind of record exists from the moment of inception. Technology, at least in its youthful stages, is quite different. It is basically a “hands-on” tinkering process; relatively little is written down at the moment of creation, making it difficult (and occasionally impossible) to later reconstruct the history of the key discoveries and their discoverers. This means that, all too often, some major figures are unintentionally slighted when credit for a scientific advance is apportioned.},
author = {Mayne, W. Harry},
booktitle = {50 Years of Geophysical Ideas},
doi = {10.1190/1.9781560802389},
isbn = {978-0-931830-73-0},
month = {jan},
publisher = {Society of Exploration Geophysicists},
title = {{50 Years of Geophysical Ideas}},
url = {https://library.seg.org/doi/book/10.1190/1.9781560802389},
year = {1989}
}
@article{Green1938,
abstract = {While the idea of making an indirect determination of average sub‐surface velocities by means of reflection profiles is far from new it is nevertheless considered worth reporting, inasmuch as it has been recently employed with fair success by the writer in areas beyond any wells that could be “shot” for direct velocity measurements. Two examples of such surface velocity profiles are described—one located in the Means Field area of Andrews County, West Texas, and the other about 8 miles south of the Refugio Field, in Refugio County on the Texas Gulf Coast.},
author = {Green, C. H.},
doi = {10.1190/1.1439508},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {oct},
number = {4},
pages = {295--305},
title = {{VELOCITY DETERMINATIONS BY MEANS OF REFLECTION PROFILES}},
url = {https://library.seg.org/doi/10.1190/1.1439508},
volume = {3},
year = {1938}
}
@book{Yilmaz2001,
abstract = {Oz Yilmaz has expanded his original volume on processing to include inversion and interpretation of seismic data. In addition to the developments in all aspects of conventional processing, this two-volume set represents a comprehensive and complete coverage of the modern trends in the seismic industry-from time to depth, from 3-D to 4-D, from 4-D to 4-C, and from isotropy to anisotropy.},
author = {Yilmaz, {\"{O}}z},
booktitle = {Investigations in geophysics},
doi = {10.1190/1.9781560801580},
isbn = {978-1-56080-094-1},
month = {jan},
publisher = {Society of Exploration Geophysicists},
title = {{Seismic Data Analysis}},
url = {https://library.seg.org/doi/book/10.1190/1.9781560801580},
volume = {10},
year = {2001}
}
@article{Cameron2007,
abstract = {We address the problem of estimating seismic velocities inside the Earth which is necessary for obtaining seismic images in regular Cartesian coordinates. The main goals are to develop algorithms to convert time-migration velocities to true seismic velocities, and to convert time-migrated images to depth images in regular Cartesian coordinates. Our main results are three-fold. First, we establish a theoretical relation between the true seismic velocities and the 'time-migration velocities' using the paraxial ray tracing. Second, we formulate an appropriate inverse problem describing the relation between time-migration velocities and depth velocities, and show that this problem is mathematically ill posed, i.e., unstable to small perturbations. Third, we develop numerical algorithms to solve regularized versions of these equations which can be used to recover smoothed velocity variations. Our algorithms consist of efficient time-to-depth conversion algorithms, based on Dijkstra-like fast marching methods, as well as level set and ray tracing algorithms for transforming Dix velocities into seismic velocities. Our algorithms are applied to both two-dimensional and three-dimensional problems, and we test them on a collection of both synthetic examples and field data. {\textcopyright} 2007 IOP Publishing Ltd.},
author = {Cameron, M. K. and Fomel, S. B. and Sethian, J. A.},
doi = {10.1088/0266-5611/23/4/001},
issn = {0266-5611},
journal = {Inverse Problems},
month = {aug},
number = {4},
pages = {1329--1369},
title = {{Seismic velocity estimation from time migration}},
url = {https://iopscience.iop.org/article/10.1088/0266-5611/23/4/001},
volume = {23},
year = {2007}
}
@article{Li2015,
abstract = {The problem of conversion from time-migration velocity to an interval velocity in depth in the presence of lateral velocity variations can be reduced to solving a system of partial differential equations. In this paper, we formulate the problem as a non-linear least-squares optimization for seismic interval velocity and seek its solution iteratively. The input for the inversion is the Dix velocity, which also serves as an initial guess. The inversion gradually updates the interval velocity in order to account for lateral velocity variations that are neglected in the Dix inversion. The algorithm has a moderate cost thanks to regularization that speeds up convergence while ensuring a smooth output. The proposed method should be numerically robust compared to the previous approaches, which amount to extrapolation in depth monotonically. For a successful time-to-depth conversion, image-ray caustics should be either nonexistent or excluded from the computational domain. The resulting velocity can be used in subsequent depth-imaging model building. Both synthetic and field data examples demonstrate the applicability of the proposed approach.},
author = {Li, Siwei and Fomel, Sergey},
doi = {10.1111/1365-2478.12191},
issn = {00168025},
journal = {Geophysical Prospecting},
month = {mar},
number = {2},
pages = {315--337},
title = {{A robust approach to time-to-depth conversion and interval velocity estimation from time migration in the presence of lateral velocity variations}},
url = {http://doi.wiley.com/10.1111/1365-2478.12191},
volume = {63},
year = {2015}
}
@article{Taner1969,
abstract = {Multifold ground coverage by seismic techniques such as the common reflection point method provides a multiplicity of wave travel path information which allows direct determination of root‐mean‐square velocities associated with such paths. Hyperbolic searches for semblance among appropriately gathered arrays of traces form the basis upon which velocities are estimated. Measured semblances are presented as a velocity spectral display. Interpretation of this information can give velocities with meaningful accuracy for primary as well as multiple events. In addition, the velocity data can help correctly label events. This paper outlines the fundamental principles for calculating velocity spectra displays. Examples are included which demonstrate the depth and detail of geological information which may be obtained from the interpretation of such displays.},
author = {Taner, M. Turhan and Koehler, Fulton},
doi = {10.1190/1.1440058},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {dec},
number = {6},
pages = {859--881},
title = {{VELOCITY SPECTRA—DIGITAL COMPUTER DERIVATION APPLICATIONS OF VELOCITY FUNCTIONS}},
url = {https://library.seg.org/doi/10.1190/1.1440058},
volume = {34},
year = {1969}
}
@article{Luo2012,
abstract = {Increasing the resolution of semblance-based velocity spectra, or semblance spectra, is useful for estimating normal moveout velocities, as increased resolution can help to distinguish peaks in the spectra. The resolution of semblance spectra depends on the sensitivity of semblance to changes in velocity. By weighting terms in the semblance calculation that are more sensitive to changes in velocity, we can increase resolution. Our implementation of weighted semblance is a straightforward extension of conventional semblance. Somewhat surprisingly, we increase resolution by choosing an offset-dependent weighting function that minimizes semblance. We test our method on synthetic and field data, and our tests confirm that weighted semblance provides higher resolution than conventional semblance.},
author = {Luo, Simon and Hale, Dave},
doi = {10.1190/geo2011-0034.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {mar},
number = {2},
pages = {U15--U22},
title = {{Velocity analysis using weighted semblance}},
url = {https://library.seg.org/doi/10.1190/geo2011-0034.1},
volume = {77},
year = {2012}
}
@article{Toldi1989,
abstract = {Conventionally, interval velocities are derived from picked stacking velocities. The velocity‐analysis algorithm proposed in this paper is also based on stacking velocities; however, it eliminates the conventional picking stage by always considering stacking velocities from the point of view of an interval‐velocity model. This view leads to a model‐based, automatic velocity‐analysis algorithm. The algorithm seeks to find an interval‐velocity model such that the stacking velocities calculated from that model give the most powerful stack. An additional penalty is incurred for models that differ in smoothness from an initial interval‐velocity model. The search for the best model is conducted by means of a conjugate‐gradient method. The connection between the interval‐velocity model and the stacking velocities plays an important role in the algorithm proposed in this paper. In the simplest case, stacking velocity is assumed to be equal to rms velocity. For the more general case, a linear theory is developed, connecting interval velocity and stacking velocity through the intermediary of traveltime. When applied to a field data set, the method produces an interval‐velocity model that explains the lateral variation in both stacking velocity and traveltime.},
author = {Toldi, John L.},
doi = {10.1190/1.1442643},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {feb},
number = {2},
pages = {191--199},
title = {{Velocity analysis without picking}},
url = {https://library.seg.org/doi/10.1190/1.1442643},
volume = {54},
year = {1989}
}
@inproceedings{Adler1999,
abstract = {We present a new and fast method for estimating dense stacking velocities of 3D seismic data. The method combines fast automated velocity picking with statistically robust 3D editing of velocities in 3D by locally scaled regression. The method provides improved lateral resolution of stack images which is illustrated with an example from the North Sea.},
author = {Adler, Frank and Brandwood, Simon},
booktitle = {SEG Technical Program Expanded Abstracts 1999},
doi = {10.1190/1.1820709},
month = {jan},
pages = {1162--1165},
publisher = {Society of Exploration Geophysicists},
title = {{Robust estimation of dense 3D stacking velocities from automated picking}},
url = {http://library.seg.org/doi/abs/10.1190/1.1820709},
year = {1999}
}
@inproceedings{Siliqi2003,
abstract = {The focusinThe focusing process for time imaging is improved drastically when high-density parameter fields are used. Large offsets, steep dips and finally the anisotropy of the subsurface revise the bases of time processing. Today, two parameters are required: velocity (V) and anellipticity ($\eta$). Picking V and $\eta$ using two-pass techniques cannot be a long-term solution. The estimation of both parameters is very sensitive to the mute function separating near to far offsets. Picking both parameters simultaneously using dense bispectral analysis overcomes this situation. We are proposing in this paper an original parameterization of the non-hyperbolic moveout, which increases the sensitivity of the analysis and allows static moveout corrections, necessary for automatic dense pickings. An intelligent QC sorting of the raw V and $\eta$ fields, based on lateral coherency of the semblance and the Dixinversion ability of local V and $\eta$ functions, prepares skeleton fields for simultaneous geostatistical filtering of both parameters.},
author = {Siliqi, R. and {Le Meur}, D. and Gamar, F. and Smith, L. and Tour{\'{e}}, J. P. and Herrmann, P.},
booktitle = {SEG Technical Program Expanded Abstracts 2003},
doi = {10.1190/1.1817745},
issn = {19494645},
month = {jan},
number = {1},
pages = {2088--2091},
publisher = {Society of Exploration Geophysicists},
title = {{High‐density moveout parameter fields V and $\eta$. Part one: Simultaneous automatic picking}},
url = {http://library.seg.org/doi/abs/10.1190/1.1817745},
volume = {22},
year = {2003}
}
@article{Lambare2004,
abstract = {Most methods for velocity macromodel estimation require considerable operator input, mainly concerning the regularization and the picking of events in the data set or in the migrated images. For both these aspects, slope tomography methods offer interesting solutions. They consider locally coherent events characterized by their slopes in the data cube. Picking is then much easier and consequently denser than in standard traveltime tomography. Stereotomography is the latest slope tomography method. In recent years it has been improved significantly, both from an algorithmic point of view and in terms of practical use. Robust and fast procedures are now available for 2D stereotomographic picking and optimization. Concerning the picking, we propose simple criteria for the selection of relevant data among the automatically picked events. This enables an accurate smooth velocity macromodel to be estimated quite rapidly and with very limited operator intervention. We demonstrate the method using a 2D line extracted from the Oseberg NH8906 data set. {\textcopyright} 2004 European Association of Geoscientists {\&} Engineers.},
author = {Lambare, G. and Alerini, M. and Baina, R. and Podvin, P.},
doi = {10.1111/j.1365-2478.2004.00440.x},
issn = {0016-8025},
journal = {Geophysical Prospecting},
month = {nov},
number = {6},
pages = {671--681},
title = {{Stereotomography: a semi-automatic approach for velocity macromodel estimation}},
url = {http://doi.wiley.com/10.1111/j.1365-2478.2004.00440.x},
volume = {52},
year = {2004}
}
@article{Hale2009,
abstract = {Smoothing along structures apparent in seismic images can enhance these structural features while preserving important discontinuities such as faults or channels. Filters appropriate for such smoothing must seamlessly adapt to variations in the orientation and coherence of image features. I describe an implementation of smoothing filters that does this and is both computationally efficient and simple to implement. Structure-oriented filters lead naturally to the computation of structure-oriented semblance, an attribute commonly used to highlight discontinuities in seismic images. Semblance is defined in this paper as simply the ratio of a squared smoothed-image to a smoothed squared-image. This definition of semblance generalizes that commonly used today, because an unlimited variety of smoothing filters can be used to compute the numerator and denominator images in the semblance ratio. The smoothing filters described in this paper yield an especially flexible method for computing structure-oriented semblance.},
author = {Hale, Dave},
journal = {CWP Report},
keywords = {seismic image smoothing semblance},
title = {{Structure-oriented Smoothing and Semblance}},
volume = {635},
year = {2009}
}
@article{Chen2015,
abstract = {Weighted semblance can be used for improving the performance of traditional semblance for specific data sets. We have developed a novel approach for prestack velocity analysis using weighted semblance. The novelty came from a different weighting criteria in which the local similarity between each trace and a reference trace is used. On the one hand, low similarity corresponded to a noise point or a point indicating incorrect moveout, which should be given a small weight. On the other hand, high similarity corresponded to a point indicating correct moveout that should be given a high weight. Our approach could also be effectively used for analyzing AVO anomalies with increased resolution compared with AB semblance. Synthetic and field common-midpoint gathers demonstrated higher resolution using the approach we developed. Applications of the proposed method on a prestack data set further confirmed that the stacked data using the similarity-weighted semblance could obtain better energy-focused events, which indicated a more precise velocity picking.},
author = {Chen, Yangkang and Liu, Tingting and Chen, Xiaohong},
doi = {10.1190/geo2014-0618.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {jul},
number = {4},
pages = {A75--A82},
title = {{Velocity analysis using similarity-weighted semblance}},
url = {https://library.seg.org/doi/10.1190/geo2014-0618.1},
volume = {80},
year = {2015}
}
@article{Gan2016,
abstract = {Direct imaging of simultaneous-source (or blended) data, without the need of deblending, requires a precise subsurface velocity model. In this paper, we focus on the velocity analysis of simultaneous-source data using the normal moveout-based velocity picking approach.We demonstrate that it is possible to obtain a precise velocity model directly from the blended data in the common-midpoint domain. The similarity-weighted semblance can help us obtain much better velocity spectrum with higher resolution and higher reliability compared with the traditional semblance. The similarity-weighted semblance enforces an inherent noise attenuation solely in the semblance calculation stage, thus it is not sensitive to the intense interference.We use both simulated synthetic and field data examples to demonstrate the performance of the similarity-weighted semblance in obtaining reliable subsurface velocity model for direct migration of simultaneous-source data. The migrated image of blended field data using prestack Kirchhoff time migration approach based on the picked velocity from the similarity-weighted semblance is very close to the migrated image of unblended data.},
author = {Gan, Shuwei and Wang, Shoudong and Chen, Yangkang and Qu, Shan and Zu, Shaohuan},
doi = {10.1093/gji/ggv484},
issn = {0956-540X},
journal = {Geophysical Journal International},
month = {feb},
number = {2},
pages = {768--779},
title = {{Velocity analysis of simultaneous-source data using high-resolution semblance—coping with the strong noise}},
url = {https://academic.oup.com/gji/article-lookup/doi/10.1093/gji/ggv484},
volume = {204},
year = {2016}
}
@article{Fomel2007,
abstract = {By estimating local event slopes in prestack seismic reflection data, it is possible to accomplish all common time-domain imaging tasks, from normal moveout to prestack time migration, without the need to estimate seismic velocities or any other attributes. Local slopes contain complete information about the reflection geometry. Once they are estimated, seismic velocities and all other moveout parameters turn into data attributes and are directly mappable from the prestack data domain into the time-migrated image domain. I develop an analytical theory, which defines the transformation from data and local slopes to the image space for different time-domain imaging operators. Computational experiments with synthetic and field data examples confirm theoretical expectations and demonstrate practical effectiveness of the proposed method.},
author = {Fomel, Sergey},
doi = {10.1190/1.2714047},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {may},
number = {3},
pages = {S139--S147},
title = {{Velocity-independent time-domain seismic imaging using local event slopes}},
url = {https://library.seg.org/doi/10.1190/1.2714047},
volume = {72},
year = {2007}
}
@article{Fomel2002,
abstract = {Plane‐wave destruction filters originate from a local plane‐wave model for characterizing seismic data. These filters can be thought of as a time–distance (T‐X) analog of frequency‐distance (F‐X) prediction‐error filters and as an alternative to T‐X prediction‐error filters. The filters are constructed with the help of an implicit finite‐difference scheme for the local plane‐wave equation. Several synthetic and real data examples show that finite‐difference plane‐wave destruction filters perform well in applications such as fault detection, data interpolation, and noise attenuation.},
author = {Fomel, Sergey},
doi = {10.1190/1.1527095},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {nov},
number = {6},
pages = {1946--1960},
title = {{Applications of plane‐wave destruction filters}},
url = {https://library.seg.org/doi/10.1190/1.1527095},
volume = {67},
year = {2002}
}
@article{Schleicher2009,
abstract = {Current time-processing algorithms often are based on one-parameter or multiparameter coherency analysis (semblance) schemes applied to the data. Such procedures, besides being computationally expensive, lead to significant uncertainties in the searched parameters. Conventional semblance methods can be avoided for a number of imaging tasks if local slopes can be extracted directly from prestack data—for example, by filtering schemes. Although the idea is not new, it has revived for various purposes, such as velocity analysis, [Formula: see text] imaging, migration to zero offset, and time migration. We propose a simple, straightforward correction to linear plane-wave destructors based on the observation that in addition to the local slope, its inverse can be extracted from the data in a fully analogous way. Combining the information of both extractions yields a simple yet effective correction to the local slopes. The naive application of linear plane-wave destructors with our correction produces high-quality results, even with a high noise level and interfering events.},
author = {Schleicher, J. and Costa, Jesse C. and Santos, Lucio T. and Novais, Amelia and Tygel, M.},
doi = {10.1190/1.3119563},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {jul},
number = {4},
pages = {P25--P33},
title = {{On the estimation of local slopes}},
url = {https://library.seg.org/doi/10.1190/1.3119563},
volume = {74},
year = {2009}
}
@article{Du2009,
abstract = {Based on difference between upgoing and downgoing wavefields in vertical seismic profiling (VSP) data, a new method to separate upgoing and downgoing wavefields using local event slopes attribute is developed. In this method, we first use Fourier Transform to obtain initially separated wavefields, and then use plane wave destruction technology to estimate local event slopes of initially separated wavefields. Finally, original VSP data is separated on the basis of local event slopes. Our proposed method, which is a kind of optimization approach in time domain, doesn't have taper effects that result from window filter band and attenuates artifacts. Results of synthetic and field data examples show that, compared with conventional method, separated wavefields using local slopes are clean and amplitudes are maintained. The interference between upgoing and downgoing wavefields can be eliminated better.},
author = {Du, Jing and Wang, Shang Xu and Liu, Guo Chang and Liu, Yang},
doi = {10.3969/j.issn.0001-5733.2009.07.021},
issn = {00015733},
journal = {Acta Geophysica Sinica},
number = {7},
title = {{VSP wavefield separation using local slopes attribute}},
volume = {52},
year = {2009}
}
@article{Ottolini1983,
author = {Ottolini, Richard},
journal = {Stanfor Exploration Project},
pages = {59--68},
title = {{Velocity independent seismic imaging: Technical report}},
volume = {37},
year = {1983}
}
@article{Zhang2016,
abstract = {Time-domain velocity and moveout parameters can be directly obtained from local event slopes, which are estimated on the prestack seismic gathers. In practice, there are always some errors in the estimated local slopes, especially in low signal-to-noise ratio (S/N) situations. Thus, subsurface velocity information may be hidden in the image domain spanned by velocity and other moveout parameters. We have developed an accelerated clustering algorithm to find cluster centers without prior information about the number of clusters. First, plane-wave destruction is implemented to estimate the local event slopes. For every sample in the seismic gathers, we obtain the estimations of velocity and its location in the image domain, according to the local event slopes. These mapped data points in the new domain exhibit the structure of groups. We represent these points by a mixture distribution model. Then, the cluster centers of the mixture distribution model are located, which correspond to maximum likelihood velocities of the main subsurface structures. Approximate velocity uncertainties bounds are used to select centers corresponding to reflections. Finally, interpolation is performed on the clustered unevenly sampled knot velocities to build the effective velocity model on regular grids. With synthetic and field data examples, we have determined that the proposed automatic velocity estimation method can give a stacking velocity model and a time migration velocity model with relatively high accuracy.},
author = {Zhang, Peng and Lu, Wenkai},
doi = {10.1190/geo2015-0313.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {jul},
number = {4},
pages = {U13--U23},
title = {{Automatic time-domain velocity estimation based on an accelerated clustering method}},
url = {https://library.seg.org/doi/10.1190/geo2015-0313.1},
volume = {81},
year = {2016}
}
@article{Fomel2007a,
abstract = {Local seismic attributes measure seismic signal characteristics not instantaneously, at each signal point, and not globally, across a data window, but locally in the neighborhood of each point. I define local attributes with the help of regularized inversion and demonstrate their usefulness for measuring local frequencies of seismic signals and local similarity between different data sets. I use shaping regularization for controlling the locality and smoothness of local attributes. A multicomponent-image-registration example from a nine-component land survey illustrates practical applications of local attributes for measuring differences between registered images.},
author = {Fomel, Sergey},
doi = {10.1190/1.2437573},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {may},
number = {3},
pages = {A29--A33},
title = {{Local seismic attributes}},
url = {https://library.seg.org/doi/10.1190/1.2437573},
volume = {72},
year = {2007}
}
@article{Liu2011,
abstract = {Unequal illumination of the subsurface highly impacts the quality of seismic imaging. Different image points receive different folds of reflection-angle illumination, which can be caused by irregular acquisition or by wave propagation in complex media. Illumination problems can deteriorate amplitudes in migrated images. To address this problem, we present a method of stacking angle-domain common-image gathers, in which we use local similarity with soft thresholding to determine the folds of local illumination. Normalization by local similarity regularizes local illumination of reflection angles for each image point of the subsurface model. This approach compensates for irregular illumination by selective stacking in the image space, regardless of the cause of acquisition or propagation irregularities. Additional migration is not required because the methodology is implemented in the reflection angle domain after migration. We use two synthetic examples to demonstrate that our method can normalize migration amplitudes and effectively suppress migration artefacts. {\textcopyright} 2010 European Association of Geoscientists {\&} Engineers.},
author = {Liu, Guochang and Fomel, Sergey and Chen, Xiaohong},
doi = {10.1111/j.1365-2478.2010.00916.x},
issn = {00168025},
journal = {Geophysical Prospecting},
month = {mar},
number = {2},
pages = {244--255},
title = {{Stacking angle-domain common-image gathers for normalization of illumination}},
url = {http://doi.wiley.com/10.1111/j.1365-2478.2010.00916.x},
volume = {59},
year = {2011}
}
@article{Liu2018,
abstract = {Local seismic event slopes contain subsurface velocity information and can be used to estimate seismic stacking velocity. In this paper, we propose a novel approach to estimate the stacking velocity automatically from seismic reflection data using similarity-weighted k-means clustering, in which the weights are local similarity between each trace in common midpoint gather and a reference trace. Local similarity reflects the local signal-to-noise ratio in common midpoint gather. We select the data points with high signal-to-noise ratio to be used in the velocity estimation with large weights in mapped traveltime and velocity domain by similarity-weighted k-means clustering with thresholding. By using weighted k-means clustering, we make clustering centroids closer to those data points with large weights, which are more reliable and have higher signal-to-noise ratio. The interpolation is used to obtain the whole velocity volume after we have got velocity points calculated by weighted k-means clustering. Using the proposed method, one obtains a more accurate estimate of the stacking velocity because the similarity-based weighting in clustering takes into account the signal-to-noise ratio and reliability of different data points in mapped traveltime and velocity domain. In order to demonstrate that, we apply the proposed method to synthetic and field data examples, and the resulting images are of higher quality when compared with the ones obtained using existing methods.},
author = {Liu, Guochang and Li, Chao and Liu, Xingye and Ge, Qiang and Chen, Xiaohong},
doi = {10.1111/1365-2478.12602},
issn = {00168025},
journal = {Geophysical Prospecting},
month = {may},
number = {4},
pages = {649--663},
title = {{Automatic stacking-velocity estimation using similarity-weighted clustering}},
url = {http://doi.wiley.com/10.1111/1365-2478.12602},
volume = {66},
year = {2018}
}
@inproceedings{Zhang2019,
abstract = {At present, the stacking velocity mainly are acquired by manual picking on velocity spectra, which is a tedious and time-consuming process due to the growing number of the seismic data, especially 3D seismic data. To improve the efficiency in velocity picking, we have developed a new automatic velocity picking method based on deep learning. We combine two different structures of artificial neural networks, the You Only Look Once (YOLO) and Long Short-Term Memory (LSTM) for constructing an algorithm of automatic velocity picking, which are designed according to two aspects: (1) The process of velocity picking may be considered as the detection of maximum coherency values associated with primary reflections in the velocity spectra. (2) The time-velocity pairs picked from velocity spectra is a time sequence. The test with real velocity spectra from a marine seismic data set demonstrates that the deep neural network of YOLO-LSTM model for velocity auto-picking is much more efficient than manual picking.},
author = {Zhang, Hao and Zhu, Peimin and Gu, Yuan and Li, Xiaozhang},
booktitle = {SEG Technical Program Expanded Abstracts 2019},
doi = {10.1190/segam2019-3215633.1},
month = {aug},
pages = {2604--2608},
publisher = {Society of Exploration Geophysicists},
title = {{Automatic velocity picking based on deep learning}},
url = {https://library.seg.org/doi/10.1190/segam2019-3215633.1},
year = {2019}
}
@inproceedings{Ma2019,
abstract = {We developed an automatic velocity picking methodology based on convolutional neural networks (ConvNets). The proposed method formalizes the picking problem into a ConvNet regression model to map the NMO-corrected seismic gather to the velocity error estimates. We also propose a data preprocessing technique to normalize the shallow and deep reflections of a CMP gather into the same moveout shape, which is a key ingredient for successful training. A synthetic example shows the feasibility and effectiveness of the proposed method.},
author = {Ma, Yue and Ji, Xu and Fei, Tong W. and Luo, Yi},
booktitle = {SEG Technical Program Expanded Abstracts 2018},
doi = {10.1190/segam2018-2987088.1},
month = {aug},
pages = {2066--2070},
publisher = {Society of Exploration Geophysicists},
title = {{Automatic velocity picking with convolutional neural networks}},
url = {https://library.seg.org/doi/10.1190/segam2018-2987088.1},
year = {2018}
}
@article{Wang2021,
abstract = {The physical basis, parameterization, and assumptions involved in root-mean-square (rms) velocity estimation have not significantly changed since they were first developed. However, these three aspects are all good targets for novel application of the recent emergence of machine learning (ML). Therefore, it is useful at this time to provide a tutorial overview of two state-of-the-art ML implementations; we have designed and evaluated classification and regression neural networks for the extraction of apparent rms velocity trajectories from semblance data. Both networks share a similar end-to-end trainable structure, except for the final layer. In the classification network, the velocity picking is performed by finding the largest amplitude trajectory through all velocity bins. The regression network, on the other hand, applies a differentiable soft-argmax function that converts the feature maps directly to apparent rms velocity values as functions of traveltime. Relative confidence maps can also be estimated from both neural networks. A large number of synthetic models with horizontal layers are created, and common-midpoint gathers are simulated from those models as training samples. Transfer learning is applied to fine-tune the networks with a small number of samples for testing with synthetic and field data from more complicated (2D) models. Tests using synthetic data show that the regression and classification networks can give reasonable velocity predictions from semblances, but the regression network gives higher accuracy.},
author = {Wang, Wenlong and McMechan, George A. and Ma, Jianwei and Xie, Fei},
doi = {10.1190/geo2020-0423.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {mar},
number = {2},
pages = {U1--U13},
title = {{Automatic velocity picking from semblances with a new deep-learning regression strategy: Comparison with a classification approach}},
url = {https://library.seg.org/doi/10.1190/geo2020-0423.1},
volume = {86},
year = {2021}
}
@inproceedings{Schmidt1992,
abstract = {State-of-the-art processing of the data of petroleum seismology requires tedious and expensive interpretation of velocity spectra. Currently this interpretation is generally performed interactively on workstations. Even so, as petroleum seismology goes from a world of two-dimensional data only, to a world where three-dimensional data are common-place, the manual interpretation of velocity spectra becomes a major bottleneck in the overall process of progressing from raw data to a stacked section. This research promises an automated approach to the interpretation of velocity spectra via synthetic neural networks. Four independent neural networks are employed: one for picking significant times, one for picking stacking velocity at the significant times found by the first network, and two for the validation of picks selected by the first two networks. In addition to the seismic data the networks need an initial velocity function. The result is a fast automatic procedure for stacking velocity picking. Rather comprehensive tests on real data from offshore Brazil indicate that this method will probably clear the bottleneck mentioned above.},
author = {Schmidt, Jurandyr and Hadsell, Frank A.},
booktitle = {SEG Technical Program Expanded Abstracts 1992},
doi = {10.1190/1.1822036},
month = {jan},
pages = {18--21},
publisher = {Society of Exploration Geophysicists},
title = {{Neural network stacking velocity picking}},
url = {http://library.seg.org/doi/abs/10.1190/1.1822036},
year = {1992}
}
@article{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
doi = {10.1162/neco.1989.1.4.541},
issn = {0899-7667},
journal = {Neural Computation},
month = {dec},
number = {4},
pages = {541--551},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
url = {https://direct.mit.edu/neco/article/1/4/541-551/5515},
volume = {1},
year = {1989}
}
@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {nov},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {https://direct.mit.edu/neco/article/9/8/1735-1780/6109},
volume = {9},
year = {1997}
}
@inproceedings{Biswas2019,
abstract = {We describe a new method based on the Machine Learning (ML) technique for normal moveout correction (NMO) and estimation of stacking velocity. A Recurrent Neural Network (RNN) is used to calculate stacking velocity directly from the seismic data. Finally, this velocity is used for NMO correction of the data. We used the Adam optimization algorithm to train the network of neurons to estimate stacking velocity for a batch of seismic gathers. This velocity is then compared with the correct stacking velocity to update the weight. The training method minimizes a cost function defined as the mean squared error between the estimated and the correct velocities. The trained network is then used to estimate stacking velocity for rest of the gathers. Here we illustrate out method on a noisy real data set from Poland. We first trained the network using only 18 percent of gathers and then used the network to calculate stacking velocity for the remaining gathers. We used these stacking velocity to perform Normal moveout correction and finally we stacked to get the post-stack seismic section. We also show comparison between the stacks generated from the two velocities.},
author = {Biswas, Reetam and Vassiliou, Anthony and Stromberg, Rodney and Sen, Mrinal K.},
booktitle = {SEG Technical Program Expanded Abstracts 2018},
doi = {10.1190/segam2018-2997208.1},
month = {aug},
pages = {2241--2245},
publisher = {Society of Exploration Geophysicists},
title = {{Stacking velocity estimation using recurrent neural network}},
url = {https://library.seg.org/doi/10.1190/segam2018-2997208.1},
year = {2018}
}
@article{Park2020,
abstract = {Velocity analysis can be a time-consuming task when performed manually. Methods have been proposed to automate the process of velocity analysis, which, however, typically requires significant manual effort. We have developed a convolutional neural network (CNN) to estimate stacking velocities directly from the semblance. Our CNN model uses two images as one input data for training. One is an entire semblance (guide image), and the other is a small patch (target image) extracted from the semblance at a specific time step. Labels for each input data set are the root mean square velocities. We generate the training data set using synthetic data. After training the CNN model with synthetic data, we test the trained model with another synthetic data that were not used in the training step. The results indicate that the model can predict a consistent velocity model. We also noticed that when the input data are extremely different from those used for the training, the CNN model will hardly pick the correct velocities. In this case, we adopt transfer learning to update the trained model (base model) with a small portion of the target data to improve the accuracy of the predicted velocity model. A marine data set from the Gulf of Mexico is used for validating our new model. The updated model performed a reasonable velocity analysis in seconds.},
author = {Park, Min Jun and Sacchi, Mauricio D.},
doi = {10.1190/geo2018-0870.1},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {jan},
number = {1},
pages = {V33--V43},
title = {{Automatic velocity analysis using convolutional neural network and transfer learning}},
url = {https://library.seg.org/doi/10.1190/geo2018-0870.1},
volume = {85},
year = {2020}
}
@inproceedings{MacQueen1967,
abstract = {This paper describes a number of applications of the 'k-means', a procedure for classifying a random sample of points in E sub N. The procedure consists of starting with k groups which each consist of a single random point, and thereafter adding the points one after another to the group whose mean each point is nearest. After a point is added to a group, the mean of that group is adjusted so as to take account of the new point. Thus at each stage there are in fact k means, one for each group. After the sample is processed in this way, the points are classified on the basis of nearness to the final means. The portions which result tend to be fficient in the sense of having low within class variance. Applications are suggested for the problems of non-linear prediction, efficient communication, non-parametric tests of independence, similarity grouping, and automatic file construction. The extension of the methods to general metric spaces is indicated.},
author = {MacQueen, J},
booktitle = {Proceedings of the fifth Berkeley Symposium on Mathematical Statistics and Probability},
title = {{Some methods for classification and analysis of multivariate observations}},
volume = {1},
year = {1967}
}
@article{Barry1975,
abstract = {Recently a new demand for demultiplexed formats has arisen in the seismic industry due to the utilization of minicomputers in digital field recording systems and because of a growing need to standardize an acceptable data exchange format.},
author = {Barry, K. M. and Cavers, D. A. and Kneale, C. W.},
doi = {10.1190/1.1440530},
issn = {0016-8033},
journal = {GEOPHYSICS},
month = {apr},
number = {2},
pages = {344--352},
title = {{RECOMMENDED STANDARDS FOR DIGITAL TAPE FORMATS}},
url = {https://library.seg.org/doi/10.1190/1.1440530},
volume = {40},
year = {1975}
}
@misc{Hagelund2017,
author = {Hagelund, Rune and Levin, Stewart A.},
booktitle = {Society of Exploration Geophysicists},
institution = {SEG Technical Standards Committee},
title = {{SEG-Y{\_}r2.0: SEG-Y revision 2.0 Data Exchange format}},
url = {https://seg.org/Portals/0/SEG/News and Resources/Technical Standards/seg{\_}y{\_}rev2{\_}0-mar2017.pdf},
year = {2017}
}
@incollection{MacKay2003,
author = {MacKay, David},
booktitle = {Information Theory, Inference, and Learning Algorithms},
isbn = {0521642981},
pages = {284--292},
publisher = {Cambridge Univ. Press Cambridge, UK},
title = {{An example inference task: Clustering}},
volume = {20},
year = {2003}
}
@inproceedings{Hamerly2002,
abstract = {We investigate here the behavior of the standard k-means clustering algorithm and several alternatives to it: the k-harmonic means algorithm due to Zhang and colleagues, fuzzy k-means. Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to find which aspects of these algorithms contribute to finding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a unified framework that introduces separate cluster membership and data weight functions. We then show that the algorithms do behave very differently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the A-harmonic means method is superior. Having a soft membership function is essential for finding high-quality clusterings, but having a non-constant data weight function is useful also.},
address = {New York, New York, USA},
author = {Hamerly, Greg and Elkan, Charles},
booktitle = {Proceedings of the eleventh international conference on Information and knowledge management - CIKM '02},
doi = {10.1145/584792.584890},
isbn = {1581134924},
pages = {600--607},
publisher = {ACM Press},
title = {{Alternatives to the k-means algorithm that find better clusterings}},
url = {http://dl.acm.org/citation.cfm?doid=584792.584890},
year = {2002}
}
@inproceedings{Ester1996,
abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLAR-ANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efficiency.},
address = {Portland, OR, USA},
author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"{o}}rg and Xu, Xiaowei},
booktitle = {Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining},
pages = {226--231},
title = {{A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise}},
year = {1996}
}
@inproceedings{Attias2000,
abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its conver-guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
author = {Attias, Hagai},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{A variational Bayesian framework for graphical models}},
year = {2000}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
doi = {10.1111/j.2517-6161.1977.tb01600.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
title = {{ Maximum Likelihood from Incomplete Data Via the EM Algorithm }},
volume = {39},
year = {1977}
}
@article{Blei2006,
abstract = {Dirichlet process (DP) mixture models are the cornerstone of non- parametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of non- parametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to ex- plore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, varia- tional methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem. {\textcopyright} 2006 International Society for Bayesian Analysis.},
author = {Blei, David M. and Jordan, Michael I.},
doi = {10.1214/06-BA104},
issn = {1936-0975},
journal = {Bayesian Analysis},
month = {mar},
number = {1},
pages = {121--143},
title = {{Variational inference for Dirichlet process mixtures}},
url = {http://projecteuclid.org/euclid.ba/1340371077},
volume = {1},
year = {2006}
}
@article{Blei2017,
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
doi = {10.1080/01621459.2017.1285773},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {apr},
number = {518},
pages = {859--877},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
volume = {112},
year = {2017}
}
@article{Blackwell2007,
abstract = {The Polya urn scheme is extended by allowing a continuum of colors. For the extended scheme, the distribution of colors after n draws is shown to converge as n -{\textgreater} infinity to a limiting discrete distribution mu*. The distribution of mu* is shown to be one introduced by Ferguson and, given mu*, the colors drawn from the urn are shown to be independent with distribution mu*.},
author = {Blackwell, David and MacQueen, James B.},
doi = {10.1214/aos/1176342372},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
title = {{Ferguson Distributions Via Polya Urn Schemes}},
volume = {1},
year = {2007}
}
@incollection{Aldous1985,
abstract = {Page 1. EXCHANGEABILITY AND RELATED TOPICS PAR David J. ALDOUS Page 2. O. Introduction If you had asked a probabilist in 1970 what was known about exchangeability , you would likely have received the answer "There's de Finetti's ...},
author = {Aldous, David J.},
doi = {10.1007/bfb0099421},
title = {{Exchangeability and related topics}},
year = {1985}
}
@misc{Sethuraman1994,
abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
author = {Sethuraman, J.},
booktitle = {Statistica Sinica},
month = {sep},
number = {3},
pages = {639--650},
title = {{A constructive definition of Dirichlet priors}},
url = {http://www3.stat.sinica.edu.tw/statistica/j4n2/j4n27/..{\%}5Cj4n216{\%}5Cj4n216.htm},
volume = {4},
year = {1994}
}
@article{Ferguson1973,
author = {Ferguson, Thomas S.},
doi = {10.1214/aos/1176342360},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {mar},
number = {2},
title = {{A Bayesian Analysis of Some Nonparametric Problems}},
url = {https://projecteuclid.org/journals/annals-of-statistics/volume-1/issue-2/A-Bayesian-Analysis-of-Some-Nonparametric-Problems/10.1214/aos/1176342360.full},
volume = {1},
year = {1973}
}
@article{Escobar1995,
abstract = {We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation and are exemplified by special cases where data are modeled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior, and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. {\textcopyright} 1995 Taylor {\&} Francis Group, LLC.},
author = {Escobar, Michael D. and West, Mike},
doi = {10.1080/01621459.1995.10476550},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
number = {430},
title = {{Bayesian density estimation and inference using mixtures}},
volume = {90},
year = {1995}
}
@article{Geman1984,
abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”) or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios. {\textcopyright} 1984, IEEE.},
author = {Geman, Stuart and Geman, Donald},
doi = {10.1109/TPAMI.1984.4767596},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {6},
title = {{Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images}},
volume = {PAMI-6},
year = {1984}
}

% Encoding: UTF-8

@Article{Rodriguez2014,
  author       = {Rodriguez, Alex and Laio, Alessandro},
  title        = {Clustering by fast search and find of density peaks},
  volume       = {344},
  number       = {6191},
  pages        = {1492--1496},
  date         = {2014},
  journaltitle = {science},
  publisher    = {American Association for the Advancement of Science},
}

@Article{Fraenti2006,
  author       = {Fr{\"a}nti, Pasi and Virmajoki, Olli},
  title        = {Iterative shrinking method for clustering problems},
  volume       = {39},
  number       = {5},
  pages        = {761--775},
  date         = {2006},
  journaltitle = {Pattern Recognition},
  publisher    = {Elsevier},
}

@Article{Fu2007,
  author       = {Fu, Limin and Medico, Enzo},
  title        = {FLAME, a novel fuzzy clustering method for the analysis of DNA microarray data},
  volume       = {8},
  number       = {1},
  pages        = {1--15},
  date         = {2007},
  journaltitle = {BMC bioinformatics},
  publisher    = {BioMed Central},
}

@Article{Chang2008,
  author       = {Chang, Hong and Yeung, Dit-Yan},
  title        = {Robust path-based spectral clustering},
  volume       = {41},
  number       = {1},
  pages        = {191--203},
  date         = {2008},
  journaltitle = {Pattern Recognition},
  publisher    = {Elsevier},
}

@Article{Franti2006,
  author       = {Franti, Pasi and Virmajoki, Olli and Hautamaki, Ville},
  title        = {Fast agglomerative clustering using a k-nearest neighbor graph},
  volume       = {28},
  number       = {11},
  pages        = {1875--1881},
  date         = {2006},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  publisher    = {IEEE},
}

@InCollection{Charytanowicz2010,
  author    = {Charytanowicz, Ma{\l}gorzata and Niewczas, Jerzy and Kulczycki, Piotr and Kowalski, Piotr A and {\L}ukasik, Szymon and {\.Z}ak, S{\l}awomir},
  title     = {Complete gradient clustering algorithm for features analysis of x-ray images},
  booktitle = {Information technologies in biomedicine},
  publisher = {Springer},
  pages     = {15--24},
  date      = {2010},
}

@InProceedings{Samaria1994,
  author       = {Samaria, Ferdinando S and Harter, Andy C},
  title        = {Parameterisation of a stochastic model for human face identification},
  booktitle    = {Proceedings of 1994 IEEE workshop on applications of computer vision},
  pages        = {138--142},
  organization = {IEEE},
  date         = {1994},
}

@Article{Sampat2009,
  author       = {Sampat, Mehul P and Wang, Zhou and Gupta, Shalini and Bovik, Alan Conrad and Markey, Mia K},
  title        = {Complex wavelet structural similarity: A new image similarity index},
  volume       = {18},
  number       = {11},
  pages        = {2385--2401},
  date         = {2009},
  journaltitle = {IEEE transactions on image processing},
  publisher    = {IEEE},
}

@InProceedings{Dueck2007,
  author       = {Dueck, Delbert and Frey, Brendan J},
  title        = {Non-metric affinity propagation for unsupervised image categorization},
  booktitle    = {2007 IEEE 11th International Conference on Computer Vision},
  pages        = {1--8},
  organization = {IEEE},
  date         = {2007},
}

@Article{Marinelli2009,
  author       = {Marinelli, Fabrizio and Pietrucci, Fabio and Laio, Alessandro and Piana, Stefano},
  title        = {A kinetic model of trp-cage folding from multiple biased molecular dynamics simulations},
  volume       = {5},
  number       = {8},
  pages        = {e1000452},
  date         = {2009},
  journaltitle = {PLoS Comput Biol},
  publisher    = {Public Library of Science},
}

@Article{Horenko2006,
  author       = {Horenko, Illia and Dittmer, Evelyn and Fischer, Alexander and Sch{\"u}tte, Christof},
  title        = {Automated model reduction for complex systems exhibiting metastability},
  volume       = {5},
  number       = {3},
  pages        = {802--827},
  date         = {2006},
  journaltitle = {Multiscale Modeling \& Simulation},
  publisher    = {SIAM},
}

@Article{Cheng1995,
  author       = {Cheng, Yizong},
  title        = {Mean shift, mode seeking, and clustering},
  volume       = {17},
  number       = {8},
  pages        = {790--799},
  date         = {1995},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  publisher    = {IEEE},
}

@Article{Gionis2007,
  author       = {Gionis, Aristides and Mannila, Heikki and Tsaparas, Panayiotis},
  title        = {Clustering aggregation},
  volume       = {1},
  number       = {1},
  pages        = {4--es},
  date         = {2007},
  journaltitle = {Acm transactions on knowledge discovery from data (tkdd)},
  publisher    = {ACM New York, NY, USA},
}

@Article{Xu2005,
  author       = {Xu, Rui and Wunsch, Donald},
  title        = {Survey of clustering algorithms},
  volume       = {16},
  number       = {3},
  pages        = {645--678},
  date         = {2005},
  journaltitle = {IEEE Transactions on neural networks},
  publisher    = {Ieee},
}

@InProceedings{MacQueen1967,
  author       = {MacQueen, James and others},
  title        = {Some methods for classification and analysis of multivariate observations},
  booktitle    = {Proceedings of the fifth Berkeley symposium on mathematical statistics and probability},
  volume       = {1},
  number       = {14},
  pages        = {281--297},
  organization = {Oakland, CA, USA},
  date         = {1967},
}

@Book{Kaufman2009,
  title     = {Finding groups in data: an introduction to cluster analysis},
  publisher = {John Wiley \& Sons},
  author    = {Kaufman, Leonard and Rousseeuw, Peter J},
  volume    = {344},
  date      = {2009},
}

@Article{Frey2007,
  author       = {Frey, Brendan J and Dueck, Delbert},
  title        = {Clustering by passing messages between data points},
  volume       = {315},
  number       = {5814},
  pages        = {972--976},
  date         = {2007},
  journaltitle = {science},
  publisher    = {American Association for the Advancement of Science},
}

@Article{Jr1963,
  author       = {Ward Jr, Joe H},
  title        = {Hierarchical grouping to optimize an objective function},
  volume       = {58},
  number       = {301},
  pages        = {236--244},
  date         = {1963},
  journaltitle = {Journal of the American statistical association},
  publisher    = {Taylor \& Francis Group},
}

@Book{Hoeppner1999,
  title     = {Fuzzy cluster analysis: methods for classification, data analysis and image recognition},
  publisher = {John Wiley \& Sons},
  author    = {H{\"o}ppner, Frank and Klawonn, Frank and Kruse, Rudolf and Runkler, Thomas},
  date      = {1999},
}

@Article{Jain2010,
  author       = {Jain, Anil K},
  title        = {Data clustering: 50 years beyond K-means},
  volume       = {31},
  number       = {8},
  pages        = {651--666},
  date         = {2010},
  journaltitle = {Pattern recognition letters},
  publisher    = {Elsevier},
}

@Book{McLachlan2007,
  title     = {The EM algorithm and extensions},
  publisher = {John Wiley \& Sons},
  author    = {McLachlan, Geoffrey J and Krishnan, Thriyambakam},
  volume    = {382},
  date      = {2007},
}

@InProceedings{Ester1996,
  author    = {Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  title     = {A density-based algorithm for discovering clusters in large spatial databases with noise.},
  booktitle = {Kdd},
  volume    = {96},
  number    = {34},
  pages     = {226--231},
  date      = {1996},
}

@Article{Fukunaga1975,
  author       = {Fukunaga, Keinosuke and Hostetler, Larry},
  title        = {The estimation of the gradient of a density function, with applications in pattern recognition},
  volume       = {21},
  number       = {1},
  pages        = {32--40},
  date         = {1975},
  journaltitle = {IEEE Transactions on information theory},
  publisher    = {IEEE},
}

@Comment{jabref-meta: databaseType:bibtex;}
