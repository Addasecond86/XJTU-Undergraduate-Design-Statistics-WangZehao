聚类算法试图根据数据之间的相似性将其分类或聚类. 目前已经有很多种不同的聚类算法\cite{Xu2005}, 但它们甚至在如何实施聚类这个问题上也有很大的不同做法. 在K-means\cite{MacQueen1967}和K-medoids\cite{Kaufman2009}方法中, 通常是以数据点到某个聚类中心的距离较小为一类的特征. 此时, 目标函数通常是到一组假定的聚类中心的距离之和, 然后对聚类中心进行优化\cite{Kaufman2009,Frey2007,Jr1963,Hoeppner1999}, 直到找到最好的簇中心. 然而, 由于数据点总是被分配到离他最近的聚类中心, 这些聚类方法无法应用于非球形簇\cite{Jain2010}. 而在基于概率分布的算法中, 人们试图将观察到的数据点表示为某些函数的概率分布\cite{McLachlan2007}, 这种方法的准确性则取决于选取的概率函数表示数据的能力. 

而具有任意形状簇的数据分布很容易被基于局部密度的聚类方法检测到. 在DBSCAN\cite{Ester1996}算法中, 我们需要选择一个密度阈值, 将密度低于这个阈值的区域中的点作为噪声丢弃掉, 将高密度的不相连的区域划分到不同的簇中. 但如何选择合适的阈值是一个比较困难的问题. 然而这个缺点在均值平移算法中得到了克服\cite{Fukunaga1975,Cheng1995}, 在这个算法中, 簇被定义为一组接近于密度分布函数的某一局部最大值的点. 这种方法可以找到非球形聚类, 但只适用于由一组坐标定义的数据, 且计算成本较高. 

因此, 我们提出了一种替代方法. 它与K-medoids方法类似, 以数据点之间的距离作为聚类特征. 像DBSCAN和均值平移法一样, 它也能够检测到非球形聚类, 并自动确定聚类中心的数量. 与均值移动法一样, 聚类中心被定义为数据密度函数的局部最大值. 然而, 与均值移动法不同的是, 它不需要将数据嵌入到一个向量空间, 也不需要具体的将每个数据点的密度最大化. 
